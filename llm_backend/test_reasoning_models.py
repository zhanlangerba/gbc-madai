#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯DeepSeekæ¨ç†æ¨¡å‹é…ç½®
ç¡®ä¿åœ¨ä¸åŒåœºæ™¯ä¸‹ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹
"""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.core.config import settings
from app.services.deepseek_service import DeepseekService
from app.services.llm_factory import LLMFactory
from app.core.logger import get_logger

logger = get_logger(service="test_reasoning")

async def test_chat_model():
    """æµ‹è¯•èŠå¤©æ¨¡å‹"""
    print("=" * 60)
    print("æµ‹è¯•èŠå¤©æ¨¡å‹ (deepseek-chat)")
    print("=" * 60)
    
    chat_service = DeepseekService(use_reasoning=False)
    print(f"èŠå¤©æœåŠ¡ä½¿ç”¨çš„æ¨¡å‹: {chat_service.model}")
    
    messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±"}]
    
    try:
        response = await chat_service.generate(messages)
        print(f"èŠå¤©å“åº”: {response[:200]}...")
        return True
    except Exception as e:
        print(f"èŠå¤©æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def test_reasoning_model():
    """æµ‹è¯•æ¨ç†æ¨¡å‹"""
    print("=" * 60)
    print("æµ‹è¯•æ¨ç†æ¨¡å‹ (deepseek-reasoner)")
    print("=" * 60)
    
    reasoning_service = DeepseekService(use_reasoning=True)
    print(f"æ¨ç†æœåŠ¡ä½¿ç”¨çš„æ¨¡å‹: {reasoning_service.model}")
    
    messages = [{"role": "user", "content": "è¯·è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ï¼Œå¹¶åˆ†æå…¶åœ¨å¯†ç å­¦ä¸­çš„åº”ç”¨å‰æ™¯"}]
    
    try:
        response = await reasoning_service.generate_reasoning(messages)
        print(f"æ¨ç†å“åº”: {response[:200]}...")
        return True
    except Exception as e:
        print(f"æ¨ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def test_factory_services():
    """æµ‹è¯•å·¥å‚åˆ›å»ºçš„æœåŠ¡"""
    print("=" * 60)
    print("æµ‹è¯•LLMå·¥å‚åˆ›å»ºçš„æœåŠ¡")
    print("=" * 60)
    
    # æµ‹è¯•èŠå¤©æœåŠ¡
    chat_service = LLMFactory.create_chat_service()
    print(f"å·¥å‚åˆ›å»ºçš„èŠå¤©æœåŠ¡æ¨¡å‹: {getattr(chat_service, 'model', 'N/A')}")
    
    # æµ‹è¯•æ¨ç†æœåŠ¡
    reasoning_service = LLMFactory.create_reasoner_service()
    print(f"å·¥å‚åˆ›å»ºçš„æ¨ç†æœåŠ¡æ¨¡å‹: {getattr(reasoning_service, 'model', 'N/A')}")
    
    return True

def test_environment_variables():
    """æµ‹è¯•ç¯å¢ƒå˜é‡é…ç½®"""
    print("=" * 60)
    print("æµ‹è¯•ç¯å¢ƒå˜é‡é…ç½®")
    print("=" * 60)
    
    print(f"DEEPSEEK_MODEL (èŠå¤©): {settings.DEEPSEEK_MODEL}")
    print(f"DEEPSEEK_REASON_MODEL (æ¨ç†): {settings.DEEPSEEK_REASON_MODEL}")
    print(f"CHAT_SERVICE: {settings.CHAT_SERVICE}")
    print(f"REASON_SERVICE: {settings.REASON_SERVICE}")
    print(f"AGENT_SERVICE: {settings.AGENT_SERVICE}")
    
    # éªŒè¯é…ç½®
    assert settings.DEEPSEEK_MODEL == "deepseek-chat", f"èŠå¤©æ¨¡å‹åº”è¯¥æ˜¯deepseek-chatï¼Œå®é™…æ˜¯{settings.DEEPSEEK_MODEL}"
    assert settings.DEEPSEEK_REASON_MODEL == "deepseek-reasoner", f"æ¨ç†æ¨¡å‹åº”è¯¥æ˜¯deepseek-reasonerï¼Œå®é™…æ˜¯{settings.DEEPSEEK_REASON_MODEL}"
    
    print("âœ… ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®")
    return True

async def test_stream_reasoning():
    """æµ‹è¯•æµå¼æ¨ç†"""
    print("=" * 60)
    print("æµ‹è¯•æµå¼æ¨ç†")
    print("=" * 60)
    
    reasoning_service = DeepseekService(use_reasoning=True)
    messages = [{"role": "user", "content": "è¯·åˆ†æäººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿"}]
    
    try:
        print("å¼€å§‹æµå¼æ¨ç†...")
        chunk_count = 0
        async for chunk in reasoning_service.generate_reasoning_stream(messages):
            chunk_count += 1
            if chunk_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªchunk
                print(f"Chunk {chunk_count}: {chunk[:50]}...")
        
        print(f"âœ… æµå¼æ¨ç†æˆåŠŸï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªæ•°æ®å—")
        return True
    except Exception as e:
        print(f"æµå¼æ¨ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹DeepSeekæ¨ç†æ¨¡å‹é…ç½®æµ‹è¯•")
    print(f"å½“å‰æ—¶é—´: {asyncio.get_event_loop().time()}")
    
    tests = [
        ("ç¯å¢ƒå˜é‡é…ç½®", test_environment_variables),
        ("å·¥å‚æœåŠ¡åˆ›å»º", test_factory_services),
        ("èŠå¤©æ¨¡å‹", test_chat_model),
        ("æ¨ç†æ¨¡å‹", test_reasoning_model),
        ("æµå¼æ¨ç†", test_stream_reasoning),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nğŸ” æ‰§è¡Œæµ‹è¯•: {test_name}")
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            results.append((test_name, result))
            print(f"âœ… {test_name}: {'é€šè¿‡' if result else 'å¤±è´¥'}")
        except Exception as e:
            print(f"âŒ {test_name}: å¼‚å¸¸ - {str(e)}")
            results.append((test_name, False))
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨ç†æ¨¡å‹é…ç½®æ­£ç¡®ã€‚")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
